\section{Conclusion}
\label{conclusion}

One previous work named FARSEC identifies security related words and applies filters to remove not-security bug reports. FARSEC build prediction models with default configurations in learners from Weka. However, FARSEC does not achieve good prediction performance. In this work, we propose to apply hyperparameter optimization to learners and use a data balancing technique called SMOTE to oversample the imbalanced class in FARSEC datasets. Furthermore, we use a tuned version of SMOTE to address the imbalanced class isse. Our experiment results on FARSEC datasets shows that hyperparameer optimization can improve performance than default configuration. However, the improvement is limited and does not apply for all datasets. SMOTE and SMOTUNED can efficient improve the baseline prediction results.